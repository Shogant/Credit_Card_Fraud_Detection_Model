{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22fffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\credit-card-fraud-detection-model-FZHIqfLr-py3.13\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b258b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ddecfc; color:#100; padding:30px; border-radius:50px; max-width:1200px; margin:left;\">\n",
    "\n",
    "# Data Exploration ‚Äì Dataset Tables\n",
    "\n",
    "In this section we will first try to explore our raw dataset files to understand \n",
    "its structure, columns, datatypes, and potential issues (missing values, duplicates, etc.), before we preform any merge.\n",
    "\n",
    "<br><u> We have 2 tables: </u></br>\n",
    "* **customers.csv**\n",
    "* **credit_card_fraud.csv**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b1746",
   "metadata": {},
   "source": [
    "### <font color = navy > <u> Data load: </u></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddad647",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"C:/Users/User/Documents/GitHub/Credit_Card_Fraud_Detection_Model/data/credit_card_fraud.csv\", index_col=0) #first column is index\n",
    "customers_df = pd.read_csv(\"C:/Users/User/Documents/GitHub/Credit_Card_Fraud_Detection_Model/data/customers.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transactions_df.shape)\n",
    "print(customers_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a3439",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f4f9fe ; color:#200; padding:30px; border-radius:50px;max-width:1200px; margin:left;\">\n",
    "\n",
    "The transaction_df has 34 million rows. Initial loading of both tables took about 6-12 minutes. <br>\n",
    "To ensure smoother experimentation and reproducibility:\n",
    "- lets store the datasets as Pickle (.pkl) files for quick pull (incase of kernel resets)\n",
    "</br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save once (binary format)\n",
    "transactions_df.to_pickle(\"transactions.pkl\")\n",
    "customers_df.to_pickle(\"customers.pkl\")\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887ce9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick load :\n",
    "transactions_df = pd.read_pickle(\"transactions.pkl\")\n",
    "customers_df = pd.read_pickle(\"customers.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630a34d",
   "metadata": {},
   "source": [
    "#### <font color='Navy'> <b><u> Functions : </b></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de3bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_columns(df1, name1, df2, name2):\n",
    "    \"\"\"\n",
    "    Compares columns between two DataFrames.\n",
    "    Prints:\n",
    "    1. Common columns and their data types in both datasets.\n",
    "    2. Columns unique to each dataset.\n",
    "    \"\"\"\n",
    "    set1, set2 = set(df1.columns), set(df2.columns)\n",
    "    common = set1.intersection(set2)\n",
    "    only_in_1 = set1 - set2\n",
    "    only_in_2 = set2 - set1\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üîπ Common Columns with Data Types\")\n",
    "    if common:\n",
    "        comparison = {\n",
    "            \"Column\": [],\n",
    "            f\"{name1} dtype\": [],\n",
    "            f\"{name2} dtype\": []\n",
    "        }\n",
    "        for col in sorted(common):\n",
    "            comparison[\"Column\"].append(col)\n",
    "            comparison[f\"{name1} dtype\"].append(df1[col].dtype)\n",
    "            comparison[f\"{name2} dtype\"].append(df2[col].dtype)\n",
    "        result = pd.DataFrame(comparison)\n",
    "        print(result.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No common columns found.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üîπ Columns only in {name1}:\")\n",
    "    if only_in_1:\n",
    "        print(sorted(list(only_in_1)))\n",
    "\n",
    "    else:\n",
    "        print(\"No unique columns found.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üîπ Columns only in {name2}:\")\n",
    "    if only_in_2:\n",
    "        print(sorted(list(only_in_2)))\n",
    "    else:\n",
    "        print(\"No unique columns found.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return common, only_in_1, only_in_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values_report(df, name, max_rows=20):\n",
    "    \"\"\"\n",
    "    Prints number of unique values for each column in a DataFrame.\n",
    "    Shows top 'max_rows' columns sorted by uniqueness.\n",
    "    \"\"\"\n",
    "    uniques = df.nunique().sort_values(ascending=False).reset_index()\n",
    "    uniques.columns = [\"Column\", \"Unique Values\"]\n",
    "    \n",
    "    print(f\"\\nüîπ Unique Value Report for {name} (showing top {max_rows}):\")\n",
    "    print(uniques.head(max_rows).to_string(index=False))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174efd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punc(col):\n",
    "    return (\n",
    "        col.str.lower()  # lowercase\n",
    "           .str.replace(r\"[^a-z0-9\\s]\", \"\", regex=True)  # remove special chars\n",
    "           .str.strip()  # remove leading/trailing spaces\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec833561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_summary(df):\n",
    "    summary = pd.DataFrame({\n",
    "        'count': df.shape[0],\n",
    "        'nulls': df.isnull().sum(),\n",
    "        'nulls%': df.isnull().mean() * 100,\n",
    "        'cardinality': df.nunique(),\n",
    "        'dtype': df.dtypes\n",
    "    })\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dee263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_case_duplicates(df, column):\n",
    "    \"\"\"\n",
    "    Checks if a column has values that differ only by letter case.\n",
    "    Returns True if normalization (lower/upper) is recommended.\n",
    "    \"\"\"\n",
    "    original_unique = set(df[column].dropna().unique())\n",
    "    normalized_unique = set(df[column].dropna().str.lower().unique())\n",
    "    \n",
    "    # If sizes differ ‚Üí case duplicates exist\n",
    "    if len(original_unique) != len(normalized_unique):\n",
    "        print(f\"‚ö†Ô∏è Column '{column}' has case duplicates.\")\n",
    "        diff = len(original_unique) - len(normalized_unique)\n",
    "        print(f\"   ‚Üí {diff} duplicate categories caused by case.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚úÖ Column '{column}' has no case duplicates.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_specials(df, column, sample_size=50000, sample=5):\n",
    "    \"\"\"\n",
    "    Checks one column for punctuation or special characters.\n",
    "    Uses a sample for speed on large datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name (string)\n",
    "    - sample_size: number of rows to sample\n",
    "    - sample: number of example values to display if issues are found\n",
    "    \"\"\"\n",
    "    # Take a sample for speed\n",
    "    df_sample = df[column].dropna().sample(\n",
    "        min(sample_size, df[column].dropna().shape[0]), random_state=42\n",
    "    ).astype(str)\n",
    "    \n",
    "    pattern = re.compile(r\"[^a-zA-Z0-9\\s]\")\n",
    "    mask = df_sample.str.contains(pattern, na=False)\n",
    "    count = mask.sum()\n",
    "    \n",
    "    if count > 0:\n",
    "        print(f\"‚ö†Ô∏è Column '{column}' has {count} rows with special characters (in sample).\")\n",
    "        print(\"   Examples:\", df_sample[mask].unique()[:sample])\n",
    "        return \"Needs Cleaning\"\n",
    "    else:\n",
    "        print(f\"‚úÖ Column '{column}' is clean (no special characters in sample).\")\n",
    "        return \"Clean\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60809c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_remark(summary_table, col_name, remark):\n",
    "    \"\"\"\n",
    "    Add a remark for a specific column in the summary_table.\n",
    "    \n",
    "    Parameters:\n",
    "        summary_table (pd.DataFrame): The summary table with 'remark' column.\n",
    "        col_name (str): Column name to mark.\n",
    "        remark (str): Remark text to add.\n",
    "    \"\"\"\n",
    "    summary_table.loc[summary_table.index == col_name, \"remark\"] = remark\n",
    "    display(summary_table.sort_values(\"cardinality\", ascending=False))\n",
    "\n",
    "\n",
    "def status_check(summary_table):\n",
    "    \"\"\"\n",
    "    Display all columns in summary_table that have no remark.\n",
    "    Works for both empty strings and NaN values.\n",
    "    \n",
    "    Parameters:\n",
    "        summary_table (pd.DataFrame): The summary table with 'remark' column.\n",
    "    \"\"\"\n",
    "    unmarked = summary_table[summary_table[\"remark\"].isna() | (summary_table[\"remark\"] == \"\")]\n",
    "    display(unmarked.sort_values(\"cardinality\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efae13",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ddecfc ;  color:#200; padding:30px; border-radius:50px;\">\n",
    "\n",
    "### 1Ô∏è‚É£ <u><b> Evaluate Data Structure </b></u>\n",
    "\n",
    "**Objective:** Understand the composition and potential issues in both datasets.\n",
    "\n",
    "**Actions:**\n",
    "- Review **shape**, **columns**, and **sample records** to understand structure.\n",
    "- Use `.info()` and `.describe()` to inspect data types and summary statistics.\n",
    "- Check for:\n",
    "  - **Name match** \n",
    "  - **Data type match**  \n",
    "  - **Value overlap**  \n",
    "  - **Uniqueness**  \n",
    "  - **Nulls**  \n",
    "  - **Duplicates**  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a07202",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c50db",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f4f9fe; color:#100; padding:20px; border-radius:10px;\">\n",
    "<font color='Navy'>\n",
    "\n",
    "It seems that we have some type of an overlap between the two sets.\n",
    "<br>\n",
    "<h3><u> Pre-Merge Checks : </u></h3>\n",
    "1. Col name match <br>\n",
    "2. Data type match<br>\n",
    "3. Uniqueness<br>\n",
    "4. Value overlap<br>\n",
    "5. Nulls<br>\n",
    "6. Duplicates  <br>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10151e42",
   "metadata": {},
   "source": [
    "#### <font color='Navy'>üîπColumns & Data types match check :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Comparison - Name and Data Types:\n",
    "common_cols, only_in_transactions, only_in_customers = compare_columns(\n",
    "    transactions_df, \"transactions\", customers_df, \"customers\"\n",
    ")\n",
    "\n",
    "#Common columns:\n",
    "print(\"Common columns:\", common_cols) \n",
    "\n",
    "# Nulls check on common columns  - do we need the merge ? is the key column complete ?\n",
    "print(\"Null counts in transactions:\")\n",
    "print(transactions_df[list(common_cols)].isna().sum())\n",
    "\n",
    "print(\"Null counts in customers:\")\n",
    "print(customers_df[list(common_cols)].isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b2cb0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f4f9fe; color:#100; padding:20px; border-radius:10px;\">\n",
    "<font color='Navy'>\n",
    "<u> Summary of comparision: </u> <br>\n",
    "</br>\n",
    "üîπNo Nulls in both data sets ‚úÖ <br> \n",
    "üîπ16 Common features : - with matching names  ‚úÖ -  matching Dtypes  ‚úÖ\n",
    "\n",
    "</br>\n",
    "üîπ <b> Columns only in transactions: </b> <br>\n",
    "- ['amt', 'category', 'is_fraud', 'merch_lat', 'merch_long', 'merchant', 'trans_date', 'trans_num', 'trans_time', 'unix_time']\n",
    "</br>\n",
    "\n",
    "============================================================\n",
    "<br>\n",
    "üîπ <b> Columns only in customers </b> - <font color='red'> No unique columns found.</font>\n",
    "\n",
    "</br>\n",
    "</font>\n",
    "\n",
    "<code>customers.csv</code> has not unique columns of its own to contribute the transaction set.<br>\n",
    "At this point, merging seems pointless.<br>\n",
    "</br>\n",
    "but before we fully dismiss it, lets see if there is any mismatch between the common values.\n",
    "lets check value overlap using the cc_num\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Lets confirm that each cc_num in customers_df appears only once (one row per customer), before we check if we have full match with transaction df.'''\n",
    "\n",
    "# Count duplicate customer IDs\n",
    "dup_customers = customers_df['cc_num'].duplicated().sum()\n",
    "\n",
    "print(\"Total customers:\", customers_df.shape[0])\n",
    "print(\"Unique customer IDs:\", customers_df['cc_num'].nunique())\n",
    "print(\"Duplicate customer IDs:\", dup_customers)\n",
    "\n",
    "# If there are duplicates, show a few\n",
    "if dup_customers > 0:\n",
    "    display(customers_df[customers_df['cc_num'].duplicated(keep=False)].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a54dfe",
   "metadata": {},
   "source": [
    "#### <font color='Navy'> üîπ Value overlap :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3163de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_cards = set(transactions_df['cc_num'])\n",
    "cust_cards = set(customers_df['cc_num'])\n",
    "\n",
    "overlap = trans_cards.intersection(cust_cards)\n",
    "\n",
    "print(\"Unique cc_num in transactions:\", len(trans_cards))\n",
    "print(\"Unique cc_num in customers   :\", len(cust_cards))\n",
    "print(\"Overlapping cc_num           :\", len(overlap))\n",
    "print(\"Overlap coverage in transactions: {:.2f}%\".format(len(overlap) / len(trans_cards) * 100))\n",
    "print(\"Overlap coverage in customers   : {:.2f}%\".format(len(overlap) / len(cust_cards) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a8273b",
   "metadata": {},
   "source": [
    "<font color='Navy'> all unique cc_nums are included in transactions dataset. Merging will only cause duplicates. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710788ff",
   "metadata": {},
   "source": [
    "#### <font color='Navy'> üîπ Duplicate Rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate rows in transactions:\", transactions_df.duplicated().sum())\n",
    "# print(\"Duplicate rows in customers   :\", customers_df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbd6dc",
   "metadata": {},
   "source": [
    "#### <font color='Navy'> üîπ Nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0380179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nulls in transactions_df :\", transactions_df.isna().sum().sum())\n",
    "# print(\"Nulls in customers cc_num   :\", customers_df['cc_num'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de6d63",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f4f9fe; color:#100; padding:20px; border-radius:10px;\">\n",
    "<font color='Navy'>\n",
    "<u> Pre-Merge Checks Summary: </u> <br>\n",
    "</br>\n",
    "\n",
    "1. Common Col Names & Dtypes - match ‚úÖ\n",
    "2. Unique features - only in transaction_df, customers features ovelap fully. üìç\n",
    "3. Value overlap - customers_df values incompassed by transaction_df. additional few customers included with no match.üìç\n",
    "5. Nulls - no nulls in both sets - ‚úÖ\n",
    "5. Duplicates - no duplicate records found in both sets. ‚úÖ\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03741679",
   "metadata": {},
   "source": [
    "Since there is no additional information given by the customer.csv, we will be working on the credict_cart_fraud.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0375f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transactions_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2b5ea",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ddecfc ;  color:#200; padding:30px; border-radius:50px;\">\n",
    "\n",
    "### 2Ô∏è‚É£ Convert and Correct Data Types\n",
    "\n",
    "**Objective:** Ensure each column is stored in the appropriate format.\n",
    "\n",
    "**Actions:**\n",
    "- Data type Corrections\n",
    "- Map Object Dtypes :\n",
    "    - tex\n",
    "- Convert date/time fields to `datetime` (`pd.to_datetime()`).\n",
    "- Convert textual categorical fields to `category` dtype.\n",
    "- Drop or isolate identifiers that are unique per record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0aacdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de7be0",
   "metadata": {},
   "source": [
    "#### <font color='Navy'> üîπValue Count </font>\n",
    "- Drop features with 1 unique values , if exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9162232",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df.nunique().sort_values()\n",
    "print(unique_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bec96",
   "metadata": {},
   "source": [
    "#### <font color='Navy'> üîπData type Correction - Date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trans_date'] = pd.to_datetime(df['trans_date'], errors='coerce')\n",
    "df['trans_time'] = pd.to_datetime(df['trans_time'], format='%H:%M:%S', errors='coerce')\n",
    "df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "# customers_df['dob'] = pd.to_datetime(customers_df['dob'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date, max_date = df['trans_date'].min(), df['trans_date'].max()\n",
    "# Print the results\n",
    "print(f\"Min Date: {min_date}\") \n",
    "print(f\"Max Date: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trans_year'] = df['trans_date'].dt.year\n",
    "df['trans_month'] = df['trans_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f75cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_fraud_counts = df_temp.groupby(['trans_year', 'trans_month', 'is_fraud']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(monthly_fraud_counts.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fafdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = monthly_fraud_counts.unstack(fill_value=0)\n",
    "result.columns = ['Non-Fraud', 'Fraud']\n",
    "display (result.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Total'] = result['Non-Fraud'] + result['Fraud']\n",
    "result['Fraud Percentage'] = (result['Fraud'] / result['Total']) * 100\n",
    "display(result.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32420ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to have Month as index and Year as columns\n",
    "result = result.reset_index()\n",
    "pivot_data = result.pivot(index='trans_month', columns='trans_year', values='Fraud Percentage')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "pivot_data.plot(marker='o', linewidth=2)\n",
    "plt.title('Fraud Percentage Comparison - 2019 vs 2020', fontsize=14)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Fraud Percentage (%)')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title='Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903fb8a",
   "metadata": {},
   "source": [
    "# Data Reduction :\n",
    "- Fraud presentage and pattern seems consistent per month for 2019 and 2020.\n",
    "- To improve computetional efficieny,  we can focus on 2020 data only for further exploration and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced Data Set:\n",
    "df_2020 = df[df['trans_year'] == 2020].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda40d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020.to_pickle(\"transactions_2020.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c996dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = pd.read_pickle(\"transactions_2020.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e720c7",
   "metadata": {},
   "source": [
    "#### <font color='Navy'> üîπDate Dtypes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract date features ---\n",
    "df['trans_year'] = df['trans_date'].dt.year\n",
    "df['trans_quarter'] = df['trans_date'].dt.quarter\n",
    "df['trans_month'] = df['trans_date'].dt.month\n",
    "df['trans_day'] = df['trans_date'].dt.day\n",
    "df['trans_dayofweek'] = df['trans_date'].dt.dayofweek\n",
    "df['trans_hour'] = df['trans_time'].dt.hour\n",
    "\n",
    "# --- Derive age ---\n",
    "# --- age of the customer at the time of transaction:\n",
    "df['age'] = (df['trans_date'] - df['dob']).dt.days // 365"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866aacba",
   "metadata": {},
   "source": [
    "#### <font color='red'> üîπData Drop List Container Definition:\n",
    "\n",
    "- Removal of original features that already split to columns like dates ( month, year, hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59727f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to track columns to drop later\n",
    "cols_to_drop = []\n",
    "\n",
    "# Example: when you identify columns to drop, append them instead of removing now\n",
    "cols_to_drop.append('trans_time')\n",
    "cols_to_drop.append('dob')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4cf15",
   "metadata": {},
   "source": [
    "#### <font color='Navy'> üîπHandle Object Data Types:\n",
    "\n",
    "- textual data convert to string\n",
    "- remove special keys\n",
    "- Classify to category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03567ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({col: 'string' for col in df.select_dtypes(include='object').columns})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strs =df_2020.select_dtypes(include='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec97af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strs.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = column_summary(df_strs)\n",
    "display(summary_table.sort_values(\"cardinality\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f1153",
   "metadata": {},
   "source": [
    "- `trans_num` has high cardinality and all are unique identifiers (not really useful)\n",
    "- `ssn` is unique identifier for the customers, no point removing '-', we might use it for group by.\n",
    "- `street`,`city`,`state` - check for granularity and category reduction.\n",
    "- `last` & `first` - consider removing.\n",
    "- `job` - reduce categories.\n",
    "- `category` - ?? keep\n",
    "- `metchant` - clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strs =df.select_dtypes(include='string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622420c5",
   "metadata": {},
   "source": [
    "Since there is no additional information given by the customer.csv, we will be working on the credict_cart_fraud.csv\n",
    "1. Duplicate rows - completed in previous section. no duplicates.\n",
    "2. Null inspection\n",
    "3. Check object columns\n",
    "4. drop if a column has only one unique value\n",
    "5. Convert an objects into viable data \n",
    "6. Clean special characters and punctioation\n",
    "7. convert gender\n",
    "8. drop first and last name\n",
    "9. group states\n",
    "10. remove street?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on transactions_df\n",
    "summary_table = column_summary(df)\n",
    "display(summary_table.sort_values(\"nulls%\", ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d97904",
   "metadata": {},
   "source": [
    "## Column Summary ‚Äì Key Insights\n",
    "\n",
    "- **No Nulls**: All columns have 0 null values ‚Üí no imputation required at this stage.  \n",
    "- **Dataset Size**: The dataset contains ~34.6M rows ‚Üí operations can be slow, so sampling may be needed for EDA.  \n",
    "- **High-Cardinality (ID-like) Columns**: `ssn`, `cc_num`, `acct_num`, `trans_num` have many unique values (up to 1 per row).  \n",
    "  - These are identifiers and not useful for EDA/modeling beyond grouping.  \n",
    "- **Categorical Columns (low/moderate cardinality)**:  \n",
    "  - `gender` (2 values), `state` (51), `category` (14), `profile` (12) ‚Üí well-suited for analysis.  \n",
    "- **Medium/High-Cardinality Categoricals**:  \n",
    "  - `city` (~5K), `job` (~600), `merchant` (~700) ‚Üí useful but may need category reduction (grouping rare values).  \n",
    "- **Datetime Columns**: `dob`, `trans_date`, `trans_time` are already converted.  \n",
    "  - Useful for deriving `age`, `year`, `month`, `day_of_week`.  \n",
    "- **Numeric Columns**:  \n",
    "  - `amt` (transaction amount), `city_pop`, `lat/long`, `merch_lat/long`.  \n",
    "  - Some have very high precision and may require binning or transformation.  \n",
    "\n",
    "**‚û°Ô∏è Next Step:** Focus on reducing categories, deriving new time-based features, and preparing the dataset for EDA visualizations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74521a48",
   "metadata": {},
   "source": [
    "# Columns Consider dropping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns where every row has a unique value\n",
    "columns_to_drop = df.columns[df.nunique() == df.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns with all unique values (to consider dropping):\", list(columns_to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb00cd0",
   "metadata": {},
   "source": [
    "### paired data checker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76975ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks whether the SSN (ssn) and credit card number (cc_num) always change together in a DataFrame (df).\n",
    "def paired_data_checker(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Checks if two columns always change together in a DataFrame.\n",
    "    Returns True if they always change together, False otherwise.\n",
    "    \"\"\"\n",
    "    paired_changes = df[[col1, col2]].drop_duplicates()\n",
    "    unique_col1 = df[col1].nunique()\n",
    "    unique_col2 = df[col2].nunique()\n",
    "    \n",
    "    if len(paired_changes) == max(unique_col1, unique_col2):\n",
    "        print(f\"{col1} and {col2} always change together. consider dropping one of them.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"{col1} and {col2} do not always change together.\")\n",
    "        return\n",
    "\n",
    "# ssn_ccnum_pairs = df[['ssn', 'cc_num']].drop_duplicates()\n",
    "# if len(ssn_ccnum_pairs) == len(df[['ssn', 'cc_num']].drop_duplicates(subset=['ssn'])):\n",
    "#     print(\"ssn and cc_num always change together. Check whether to delete the column.\")\n",
    "# else:\n",
    "#     print(\"ssn and cc_num do not always change together.\")\n",
    "# #############  df = df.drop(columns=['ssn']) ############    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51119e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_data_checker(df, 'ssn', 'cc_num')\n",
    "paired_data_checker(df, 'acct_num', 'cc_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4433df",
   "metadata": {},
   "source": [
    "## Handle objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({col: 'string' for col in df.select_dtypes(include='object').columns})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ssn'].head(5) --> 750-09-7342\n",
    "df['ssn'] = df['ssn'].str.replace('-', '', regex=False)\n",
    "df['ssn'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73166ab",
   "metadata": {},
   "source": [
    "###Geneder conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'F' to 1 and 'M' to 0\n",
    "df['gender'] = df['gender'].replace({'F': '1', 'M': '0'}).astype(int)\n",
    "df['gender'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df['city']\n",
    "df_city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city2 = df_city.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b78d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_case_duplicates(df, \"city\")\n",
    "# check_case_duplicates(df, \"state\")\n",
    "check_case_duplicates(df, \"job\")\n",
    "\n",
    "'‚úÖ Columns have no case duplicates.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081e1937",
   "metadata": {},
   "source": [
    "# Handling strings columns:\n",
    "\n",
    "1. Columns to remove - create variable that marks them ( actual removal can be later)\n",
    "2. Extraction of additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98997714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strs =df.select_dtypes(include='string')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f209d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e4578",
   "metadata": {},
   "source": [
    "## 1) Cardinality Inspection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c47020",
   "metadata": {},
   "source": [
    "2.1 Job Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on transactions_df\n",
    "summary_table = column_summary(df_strs)\n",
    "display(summary_table.sort_values(\"cardinality\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859fa8a3",
   "metadata": {},
   "source": [
    "# 1) Columns to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc38a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "removal_columns = [\"ssn\", \"first\", \"last\", \"street\", \"trans_num\"]\n",
    "#ssn - semsitive, no predictive power, unique per person\n",
    "#first, last - no predictive power\n",
    "#street - high cardinality, no predictive power\n",
    "#trans_num - unique per transaction, no predictive power\n",
    "summary_table.loc[summary_table.index.isin(removal_columns), \"remark\"] = \"remove\"\n",
    "display(summary_table.sort_values(\"cardinality\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebf262",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_check(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69625d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_strs[\"job\"].str.lower().str.strip().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0202b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = df_strs[\"job\"].unique().tolist()\n",
    "print(unique_vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_jobs(df, column, new_column=\"job_category\"):\n",
    "    \"\"\"\n",
    "    Categorizes jobs based on keywords in the job title.\n",
    "    Adds a new column with high-level categories.\n",
    "    \"\"\"\n",
    "\n",
    "    def assign_category(job):\n",
    "        job = str(job).lower()  # ensure lowercase\n",
    "        if any(word in job for word in [\"nurse\", \"doctor\", \"surgeon\", \"dentist\", \"therapist\", \"pharmacist\", \"psychiatrist\", \"psychologist\", \"radiographer\", \"optician\", \"midwife\", \"paramedic\", \"biomedical\", \"oncologist\", \"immunologist\", \"pathologist\", \"health\"]):\n",
    "            return \"healthcare\"\n",
    "        elif any(word in job for word in [\"teacher\", \"lecturer\", \"professor\", \"educator\", \"education officer\", \"tutor\", \"school\"]):\n",
    "            return \"education\"\n",
    "        elif any(word in job for word in [\"engineer\", \"technician\", \"technologist\", \"architect\", \"surveyor\", \"scientist\", \"geologist\", \"chemist\", \"biologist\", \"researcher\", \"ecologist\", \"mathematician\", \"statistician\", \"physicist\", \"astronomer\"]):\n",
    "            return \"science/engineering\"\n",
    "        elif any(word in job for word in [\"lawyer\", \"barrister\", \"solicitor\", \"attorney\", \"legal\", \"judge\", \"magistrate\"]):\n",
    "            return \"legal\"\n",
    "        elif any(word in job for word in [\"accountant\", \"finance\", \"banker\", \"economist\", \"trader\", \"investment\", \"treasurer\", \"auditor\", \"actuary\"]):\n",
    "            return \"finance\"\n",
    "        elif any(word in job for word in [\"artist\", \"designer\", \"animator\", \"illustrator\", \"musician\", \"actor\", \"writer\", \"journalist\", \"editor\", \"photographer\", \"producer\", \"curator\", \"painter\", \"sculptor\", \"filmmaker\"]):\n",
    "            return \"arts/media\"\n",
    "        elif any(word in job for word in [\"manager\", \"consultant\", \"officer\", \"administrator\", \"coordinator\", \"executive\", \"director\", \"chief\", \"ceo\", \"cfo\", \"cio\", \"cto\", \"cmo\", \"coo\"]):\n",
    "            return \"management/business\"\n",
    "        elif any(word in job for word in [\"armed forces\", \"navy\", \"army\", \"air force\", \"military\", \"police\", \"firefighter\", \"security\"]):\n",
    "            return \"public safety/military\"\n",
    "        elif any(word in job for word in [\"agricultural\", \"farm\", \"horticulturist\", \"fisheries\", \"forester\", \"conservation\", \"ecologist\", \"gardener\", \"landscaper\", \"tree surgeon\"]):\n",
    "            return \"agriculture/environment\"\n",
    "        elif any(word in job for word in [\"it\", \"software\", \"developer\", \"programmer\", \"data scientist\", \"web\", \"computer\", \"cyber\", \"network\", \"systems\", \"applications\", \"ai\", \"machine learning\"]):\n",
    "            return \"technology\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "\n",
    "    # apply categorization\n",
    "    df[new_column] = df[column].apply(assign_category)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538156c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = categorize_jobs(df, \"job\")\n",
    "df[[\"job\", \"job_category\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strs =df.select_dtypes(include='string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_remark(summary_table, \"job\", \"Reduced Cardinality to job_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_check(summary_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e54923",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant= df_strs['merchant']\n",
    "merchant.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceeab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_column_specials(df_strs, 'merchant', sample_size=10000, sample=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c361f0",
   "metadata": {},
   "source": [
    "### Issues with 'merchant':\n",
    "- High cardinality (many unique values)\n",
    "- High granularity \n",
    "- Punctionation and special keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ad527",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['merchant'] = df['merchant'].str.replace('fraud_', '').str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d22b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['merchant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6620e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df_prep_str.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48dc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df_prep_str.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_check(summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36feebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6632ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_stats = (\n",
    "    df.groupby('merchant')\n",
    "      .agg(total_transactions=('merchant', 'count'),\n",
    "           total_fraud=('is_fraud', 'sum'))\n",
    "      .reset_index().sort_values(by='total_fraud', ascending=False))\n",
    "\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac1f7c",
   "metadata": {},
   "source": [
    "Profile Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"profile\"].head(10))   # shows first 20 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the .json ending\n",
    "df['profile'] = df['profile'].str.replace('.json', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26be1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['profile'].head(10))   # shows first 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28123144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['profile'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age_group\"] = df[\"profile\"].str.split(\"_\").str[:2].str.join(\"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age_group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41406676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"location_profile\"] = df[\"profile\"].str.split(\"_\").str[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cad41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"location_profile\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df_prep_str.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4677d9",
   "metadata": {},
   "source": [
    "## 2) Extraction of additional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_stats = (\n",
    "    df.groupby('merchant')\n",
    "      .agg(total_transactions=('merchant', 'count'),\n",
    "           total_fraud=('is_fraud', 'sum'))\n",
    "      .reset_index().sort_values(by='total_fraud', ascending=False))\n",
    "\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_column_specials(df, \"city\")\n",
    "# check_column_specials(transactions_df, \"job\")\n",
    "# check_column_specials(transactions_df, \"merchant\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f468cdc",
   "metadata": {},
   "source": [
    "# Extraction of additional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a3c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f368902d",
   "metadata": {},
   "source": [
    "# Data Drops:\n",
    "- Street? high grnularity\n",
    "- First and Last name\n",
    "- - Categorical Columns where all values are unique.\n",
    "- Columns with only 1 unique value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e12e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# When ready, drop all at once\n",
    "# df_cleaned = df.drop(columns=cols_to_drop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-card-fraud-detection-model-FZHIqfLr-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
